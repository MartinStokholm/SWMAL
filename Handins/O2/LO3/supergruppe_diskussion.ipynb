{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Supergruppe diskussion\n",
    "\n",
    "\n",
    "## ยง 2 \"End-to-End Machine Learning Project\" [HOML]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume: Look at the Big Picture\n",
    "\n",
    "The first chapter explains what machine learning is in general through an example on how to predict the median of housing prices in California based on population data. It explains that you need to frame the problem correctly, so you are able to select the correct algorithms, which performance measure is used to evalute the model and how much effort is needed to tweak it. What kind of learning is needed - Supervised, unsupervised or Reinforcement? What kind of task is it a classification, a regression or other? And which learning technice is used batch og online. Example in this chapter is a typical supervised learning task, that has a typical regression task that is a mulivariation regression problem and is plain batch learning.\n",
    "\n",
    "The chapter also introduces the concept of a data pipeline which are a sequence of data processing components and are very common in Machine Learning systems to process and transform data asynchronously. The District Pricing is an example of a component. A pipeline makes for a good arcitecture but a broken component can be hard to detect if monitoring isn't done.\n",
    "\n",
    "After framing the problem you need to select a performance measure. Root Mean Square Error (RMSE) is typically a suitable option when working with a regression model as it measures the standard deviation of the errors of predictions. Mean absolute error (MAE) can also be used but is more commonly used in situations where there are numerous outliers.\n",
    "\n",
    "The chapter also highlights the importance of validating assumptions about the project since errors in these assumptions can lead to incorrect decisions later in the process. For instance, the author investigates whether the downstream machine learning system that receives housing price predictions actually uses the prices as they are or converts them into categories. This critical thinking and problem clarification are essential to avoid wasting resources and time in the project.\n",
    "\n",
    "\n",
    "#### Resume: Get the Data\n",
    "\n",
    "The section concludes by describing how to set up a isolated environment to avoid conflicting libraries by using pip and setting up a jupyter notebook. Then how to get and load the data. Next up is taking a look at the data to see what kind of data you are working with. Are all attributes numerical or are some text. Are some colums repitive and then therefore presumably a categorical attribute and so on. Transforming the dataset into histograms to get a visual reprentation of the data to see if anything cathes the eye. \n",
    "\n",
    "After looking at the data it's time to create a test set for machine learning. This is to avoid discovering patterns in the dataset and then choose a ML model that fits perfectly which is not good. \n",
    "There are various methods for splitting the data into a training set and a test are explained and what problems can arise by using the one or the other. It is explained that you need to ensure that the dataset is stratified so different categories are representative in the dataset.\n",
    "\n",
    "There are many different ways to gather data from which might be available in an online database, or might have to be collected by one self. It really depends on what you are making a machine learning model for. \n",
    "\n",
    "In summary, this section is about preparing data and establishing a test set to ensure that the machine learning system is trained and evaluated correctly.\n",
    "\n",
    "\n",
    "#### Resume: Explore and Visualize the Data to Gain Insights,\n",
    "\n",
    "In this phase of the project, the focus is jumping deeper in to exploring and visualizing the dataset. It is best practice to isolate the dataset by making a copy so you dont ruin the data by accident. As there was touced on in the previous chapter the brain is good at finding visal patterns. For this scatterploting is a good idea to identify patterns in the dataset. Playing arround with the alpha option may make it easier to spot things. Also changing the colors by using the predifinde color map jet can make it easier to visualize. The example in the book shows that you're able to spot that the housing prices are very much related to the location and population density. Next you look to find correlations between the attributes where the coefficents can range between -1 and 1. The closer to 1 the coefficent is the stronger the correlation is. Close to -1 means a strong negative correlation and close to zero means there are no linear correlation. For example The correlation coefficent only measures liner correlations why you also should look at the data for other correlations as with Panda's scatter_matrix function.\n",
    "\n",
    "Additionally you should carry out experiments with combinations of attributes to uncover new associations. This step is crucial for gaining insights into the data before feeding it to machine learning algorithms, and it contributes to making informed decisions regarding data cleaning and feature engineering.   \n",
    "\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "In this step, the data is prepared for machine learning algorithms. The author advocates using functions so you are able to reproduce these transformations and to build your own library to make your life easier when working with ML. Next is to perform data cleaning. You might get a data set where values are missing. Then you need to decide what to do. You can either drop the corresponding districts, drop the attribute or set values(zero, the mean, the median, etc.) If filling in the values you need to compute this. In other situations the attribute might be a text or a categorical attribute. In the chapter it uses the ocean_proximity to explain what your options are. If there only was two options for example ocean and inland you could use 1 for ocean and 0 for inland. If more than two option you could either give them seperate numbers but this might confuse the ML algorithms to \"think\" that two nearby values are more similar but in fact they are more distant. So it assumes that 1 and 2 are more similar but in fact it was 1 and 4 who were most similar. Instead you could transform it into a binary attribute, so 1 for ocean and otherwise 0.   \n",
    " \n",
    "Additionally you can't just rely on the Scikit-Learn library. You often need to do some custom cleanup yourself.\n",
    "\n",
    "Next a crucial part in machine learning is feature scaling, because algorithms generally struggle when dealing with input of numerical attributes that have significantly different scales. Two common methods for achieving consistent scaling across all attributes are min-max scaling, which shifts and rescales values to range from 0 to 1, and standardization, which subtracts the mean and divides by the variance to maintain a zero mean and unit variance. \n",
    "\n",
    "Lastly is crucial to perfom the transformation steps in the correct order. Scikit-Learns pipeline class can help you with this process.\n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "\n",
    "Selectiong a model is a crucial phase in the machine learning workflow. During this step, you choose a machine learning algorithm, train it on your prepared data, and assess its performance. This typically involves starting with a basic model. In the chapter it first uses a Linear Regression - but because there are som predictions that are off by more than 50% its worth looking at another model. The next another model tried is the DecisionTreeRegressor. This model is good at finding nonlinear relationships. After it's been trained it shows it has no errors but this is not necessarily true.\n",
    "\n",
    "Before launching the model on the test set you need to evaluate the model for issues like underfitting or overfitting. Cross-validation is often used to ensure robust performance assessments. This is done by dividing the traning set into smaller pieces and then training it. The book uses the K-fold cross-validation where it evaluates the Decision Tree 10 times. The cross-validation actully shows that the model performes even worse than the Linear Regression. So lastly a third model, RandomForestRegressor, is tried. In short this models trains many Decision Trees on random subsets of the features and then averaging out the predictions. It performs better but there is still some way to go. \n",
    "\n",
    "The ultimate goal is to find the best model for your specific problem and dataset before proceeding with the training process. This means finding two-five promising models to experiment with.\n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "In this section, the chapter delves into the intricacies of refining machine learning models. After shortlisting promising models, the focus shifts towards optimization, primarily through two methods: Grid Search and Randomized Search.\n",
    "\n",
    "Grid Search entails systematically experimenting with various combinations of hyperparameter values to identify the optimal configuration. Although this process can be time-intensive, tools like Scikit-Learn's GridSearchCV streamline it by evaluating different hyperparameter combinations through cross-validation.\n",
    "\n",
    "On the contrary, Randomized Search randomly selects values for hyperparameters in each iteration, proving to be more efficient when dealing with a vast hyperparameter search space.\n",
    "\n",
    "Once the best model is identified, further enhancements can be made by creating an ensemble, combining it with other models. This approach is particularly effective when individual models exhibit different types of errors.\n",
    "\n",
    "To assess a model's performance, analyzing feature importances is crucial for identifying key attributes influencing predictions. This information guides decisions on feature selection and cleaning. Additionally, scrutinizing the specific errors made by the model offers valuable insights for further performance improvement.\n",
    "\n",
    "After fine-tuning the model, it undergoes testing on the test set. It is essential to consider a confidence interval for the generalization error. While test set performance might slightly lag behind cross-validation due to fine-tuning, it is advised not to adjust hyperparameters solely for test set improvement. In the pre-launch phase of the project, summarizing the work, presenting key findings, and acknowledging system limitations are crucial. Clear documentation and visual presentations play a pivotal role. Even if the model doesn't surpass expert performance, its value lies in freeing up their time for more engaging tasks.\n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "This chapter highlights the readiness of the system for real-world deployment. To ensure ongoing success, it emphasizes the necessity of crafting monitoring code. This code plays a critical role in keeping tabs on the system's performance, stepping in when there's a dip. The goal is not just to catch sudden breakdowns but also to address the common issue of performance degradation, a natural occurrence as models age without regular training on fresh data.\n",
    "\n",
    "In addition to the technical aspects, human evaluation becomes integral to assess the system's predictions. Whether from field experts or through crowdsourcing platforms like Amazon Mechanical Turk, incorporating this human element into the evaluation pipeline is crucial.\n",
    "\n",
    "Furthermore, evaluating the quality of the system's input data is emphasized. Subtle degradation in performance may occur due to poor-quality signals, making it essential to monitor inputs, particularly for online learning systems.\n",
    "\n",
    "The chapter underscores the importance of regular model training with fresh data, advocating for an automated approach. This prevents the system's performance from fluctuating significantly over time. Notably, for online learning systems, the recommendation is to save snapshots of the system's state at regular intervals. This practice facilitates an easy rollback to a previous functional state if necessary.\n",
    "\n",
    "In essence, the chapter outlines the key steps post-approval, emphasizing the readiness of the system for real-world challenges, the importance of vigilant monitoring, human evaluations, and regular model updates for sustained and optimal performance.\n",
    "\n",
    "#### Resume: Try It Out!.\n",
    "\n",
    "The author hopes that he now has guided the reader in how a ML project looks like by showing some of the tools for training a great system.\n",
    "Most of the work lies within the preparation of the data to ensure the data is clean and suitable for ML model. Next is building monitoring tools to assess the ongoing performance. Become comfortable with just three or four algorithms to dive deep into the process rather than superficial exploration. And then just jump into and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
